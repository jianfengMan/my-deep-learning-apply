局部感知仅仅理解到这个层面是远远不够的。针对局部感知问题，我们再来看一下深度学习中的正则化方法——**参数共享**。

实际上，深度学习在进行卷积运算时，我们往往需要设置卷积核的大小以及输出的通道数，以 Caffe 为例，一个卷积层的基本配置如下：

```python
layer {
  name: "conv1" #该层的名字
  type: "Convolution"    #该层的类型，说明该层是卷积层
  bottom: "pool1"  #该层输入数据blob的名字
  top: "conv1"    #该层输出数据blob的名字
 #该层的权值和偏置相关参数
  param{
    lr_mult: 1 #weight的学习率1，和权值更新相关
  }
  param {
    lr_mult: 2 #bias的学习率2，和权值更新相关
  }
  convolution_param {
    num_output: 50   # 50个输出的map
    kernel_size: 5   #卷积核大小为5*5
    stride: 1   #卷积步长为1
    weight_filler { #权值初始化方式
      type: "xavier" #默认为constant,
      #值全为0，很多时候我们也可以用xavier
      #或者gaussian来进行初始化
    }
    bias_filler {  #偏置值的初始化方式
      type: "constant" #该参数的值和weight_filler类似，
      #一般设置为constant，值全为0
    }
  }
}
```

参数共享是指从一个局部区域学习到的信息，应用到图像的其它地方去。即用一个相同的卷积核去卷积整幅图像，相当于对图像做一个全图滤波。



通过局部感知和权值共享的方法来进行网络特征抽取，具有哪些优点呢？总结来看，主要体现在以下几个方面：

1. 抽取图像局部特征，而全局特征可以通过底层局部特征向上抽象获取，高层特征对于仿射变换不敏感；
2. 减低参数规模，实现网络的正则化；
3. 减少模型大小，降低内存消耗；
4. 减小模型的计算量。

通过参数共享，能够有效地减少参数的规模，在实际的卷积运算中，我们需要学习参数个数为：

kernelw∗kernelh∗channel