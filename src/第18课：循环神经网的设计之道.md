### 循环神经网的设计经验

针对不同的任务，在确定好网络类型之后，就要开始设计网络结构了。设计时需要考虑哪些因素，我们接下来逐一分析。

**1. 网络单元基础选型**

比较常用的循环神经网络基础单元结构有 RNN、LSTM，通常较多采用 LSTM 作为基本网络结构单元，因为 LSTM 能够在一定程度上缓解长期依赖问题。

**2. 单向 VS 双向**

双向 RNN 能够学到上下文之间的关系，可以利用未来信息，与单向 RNN 相比效果更多。另外，双向 RNN 虽然没有增加参数量，但却增加了计算量。因此，在需要考虑计算量的场景下，可以从性能角度考虑选择两者中的哪个。在不考虑算力的情况下，通常选择双向 RNN。

**3. 单层 VS 多层**

多层 RNN 与多层卷积有着类似的原理。多层 RNN 能够抽取到更高层的潜在语义信息，特征的鲁棒性通常会更好，因此多层 RNN 比单层 RNN 效果更优。

**4. 隐藏层单元数定义**

通常，对于 RNN 网络，隐藏层单元数目关系到网络特征输出的维度。结合我的经验，隐藏层单元数目通常定义为 128、256、512，更高维度意味着更高的计算量和特征的稀疏化。另外，隐藏层单元数的定义同样需要考虑序列的长度。

**5. Loss 选择**

在常见的分类、回归等任务中，Loss 的选择同卷积神经网的选择基本一致，依然是常见的 Softmax Loss、Cross Entropy Loss、L1 Loss、L2 Loss 等，而在序列到序列的任务中，建议使用 CTC Loss，以及和序列任务相关的 Loss。

**6. 初始学习率和衰减率**

设计循环神经网初始学习率和衰减率时，通常需要注意以下问题：

1. 如果网络 Loss 出现 NaN，则考虑调小学习率；
2. 学习率一般要随着训练进行衰减，衰减系数一般是 0.5（同时需要考虑样本规模），衰减时机，可以选择在验证集准确率不再上升时，或固定训练多少个周期以后；
3. 如果 RNN 要处理的序列比较长，或者 RNN 层数比较多，学习率一般小一些比较好，否则有可能出现结果不收敛，甚至 NaN 等问题；
4. 在实际计算梯度时，可以考虑梯度裁剪，尤其在网络优化的初期，梯度裁剪能够有效防止梯度爆炸。

在 TensorFlow 中，通常定义梯度裁剪的方法如下：

```python
optimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.5)
grads, variables = zip(*optimizer.compute_gradients(loss))
grads, global_norm = tf.clip_by_global_norm(grads, 5)
train_op = optimizer.apply_gradients(zip(grads, variables))
```

**7. Dropout 层**

在介绍 Dropout 层时曾提到，Dropout 通常会用在 FC 层。比较 FC 层和 RNN 的网络层时就会发现，RNN 层的主要组成部分同样由线性变换构成，因此，Dropout 在 RNN 网络中，同样能够起到有效防止过拟合的问题。通常 Dropout 取值为 0.3、0.5、0.7。

**8. 是否考虑正则项**

无论是在卷积神经网还是在循环神经网中，正则项一直都是防止网络过拟合的有效手段之一。因此，在实际工程中，建议大家加上正则项，将其作为 Loss 约束的一种。

```python
reg_set=tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES) 
loss=tf.add_n(reg_set)
```

**9. Embeding 层与输入数据**

在卷积神经网中，我们需要通过各种手段对数据进行增强。而在循环神经网络中，原始数据通常为文本数据。将一句话作为一个完整序列，如果将句子中的每个词语（也就是所谓的时间片信息或者序列信息）使用向量来描述或表示，通常会用词嵌入的方式来完成。此时，可以采用 Word2vec 进行词向量的训练和抽取，也可以采用字典编码结合 Embeding 层的方法来实现。

```python
tf.nn.embedding_lookup(params, ids, partition_strategy='mod', name=None, validate_indices=True, max_norm=None)
```

**10. 优化函数选取**

优化函数建议使用自适应梯度的方法，例如 Adam、AdaDelta、RMSprop 等。使用时，一般采用相关论文中提供到默认值即可，从而避免再花时间调学习率了。

**11. 关于正负样本比例问题**

正负样本比例问题通常会出现在语音分类、文本分类、情感分类等分类任务中。在训练的时候，尽量保证每个 Batch 中的正负样本平衡，如果负样本数量较少，则需要对其进行过采样，比如进行复制操作，提高其所占比例。或者采用在线的难例挖掘方法，找到难分的样本，进行模型调优。

### 循环神经网的调参技巧

在实际工程中，调整网络参数是一个相对比较麻烦的环节，且需要长时间的经验积累。下面总结了一些常见的循环神经网络的调参技巧（同样适用于卷积神经网络）。

**1. 模型、数据初步核对**

实际上，在很多深度学习任务中，网络不收敛很大一部分原因是数据存在异常，可能是打包时存在错误，也可能是数据本身存在异常。这个时候首先需要确保数据没有问题。如果存在问题，就需对样本进一步清洗。

在确保数据无误的情况下，再对模型进行核对，主要确定网络层参数配置是否正确，比如 Caffe 中 BatchNorm 层的使用等。确保网络没有问题之后，再进行后续分析。

**2. 小样本速调模型**

确保数据和网络搭建的模型没有错误之后，接下来就要确定模型方法是否有效。这里建议大家直接采用少量样本训练模型，保证模型在少量样本的训练集上出现过拟合问题。到这步，我们证明了网络结构以及方法是没有问题的。如果模型不收敛，此时需要考虑网络的各种参数和相关配置是否合理，通常需要考虑的有：

1. 方法是否存在正负样本的失衡问题？
2. 主干网络是否过于简单？
3. 学习率设置是否过小或者过大？
4. Batchsize 是否过小？
5. 方法是否存在问题或者方案存在问题？

**3. 大样本训练模型**

在确定模型方案没有问题之后，接下来就到了加大样本量进行模型粗调阶段。在这个阶段需要观察以下几个信息：

1. 验证集和训练集 Loss 的收敛情况；
2. 对比学习率对训练性能的影响，通常初始学习率设置为 0.1、0.01、0.001，另外，需要配合梯度裁剪方法来约束初始学习率过大，梯度较大的问题，以防止梯度爆炸的现象发生；
3. 模型的初始参数，尽量参照原始论文。如果自己设计的模型，则考虑使用 Uniform 均匀分布初始化、Normal 高斯分布初始化、SVD 初始化 

**4. 关于 Loss 曲线分析**

绘制验证集和训练集上的 Loss 曲线，以此观察模型的收敛情况。

如果训练到最后，训练集、测试集准确率都很低，则说明模型有可能欠拟合。那么接下来调节参数的方向，就是增强模型的拟合能力，例如增加网络层数、增加节点数、减少 Dropout 值，减少 L2 正则值，等等。

如果训练集准确率较高，测试集准确率比较低，那么模型有可能过拟合，这个时候就需要向提高模型泛化能力的方向，调节参数。

**5. 如何自动化调参**

人工调参的过程十分麻烦，因此可以考虑采用自动调参的方式找到最优参数组合。通常的做法有以下几种。

1. Gird Search：每种参数确定几个取值，然后组合参数，进行遍历，找到最优解，缺点则是耗时严重。
2. Random Search： 每次从所有候选参数中随机选择几个进行训练。Bengio 在《Random Search for Hyper-Parameter Optimization》中指出，Random Search 比 Gird Search 更有效。
3. Bayesian Optimization：考虑到了不同参数对应的实验结果值，更节省时间。参考文献有《Practical Bayesian Optimization of Machine Learning Algorithms》。